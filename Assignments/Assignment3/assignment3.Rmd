---
title: "Assignment 3 PCA"
author: "Nicholas Jacob and Yechang Qi"
date: "2024-09-05"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(corrplot)
library(ggfortify)
library(MASS)
library(caret)
library(magrittr)
library(dplyr)
library(HSAUR2)
library(outliers)
library(ggbiplot)
```

## Glass Data

1.a.i We'll load the data directly from our machine.  We've added the names from the info included in the zip file with the data.

```{r, include =TRUE}
names <- c('Id', 'RI','Na','Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type')
Glass <- read.csv('glass.data',header = FALSE)
names(Glass)<- names
Glass$Type = as.character(Glass$Type)
head(Glass)
```

Let's double check that there isn't a duplicate since there was a warning about a possible dup.
```{r}
any(!duplicated(Glass))
```

Does not appear to be any duplicates in this dataset.  

Now we'll create the correlation matrix.  We should remove the 'Id' and 'Type' from the data before we do the correlation.  Originally we saw strong correlation between Id and Type only because the data was ordered by type

```{r}
corMat <- cor(Glass[,2:10])
corMat
```
No ridiculously strong correlations but RI-CA is about 0.8 so it is fairly strong.

1.a.ii. Compute the eigen values and vectors of this matrix.

```{r}
eMat <- eigen(corMat)
eMat
```

As expected for the correlation matrix, all values are positive.  Eigen vectors don't show me much yet.

1.a.iii.  Next we'll do the principle component decomposition

```{r}
pMat <- prcomp(Glass[,2:10],scale = TRUE)
pMat
```
We see RI and Ca well represented in the first PCA vector.

1.a.iv. The evectors and the PCA are different.  The correlation is a scaled version of the covariance matrix so essentially, these are built off of similar matrices up to a rescaling.  If before building the correlation e-values, we did a $z$-score rescaling they would be exactly the same.  Instead as we will see in the next section, they both form the same ortho-normal basis.

1.a.v.  Show that these vectors are orthogonal to each other.

First, we needed to see how to access the vectors
```{r}
eMat$vectors[,1] #
```

We did not find a function for doing the inner product but since the formula is 
$$
x\cdot y = \sum_{i=1}^nx_iy_i
$$
We simply asked R to do that with base packages.
```{r}
sum(pMat$rotation[,1]*eMat$vectors[,1])
```

We see that this one is indeed 1 (perhaps in absolute value) as expected.  Let's build a matrix of all $9^2$ possible combinations.  Yes some are repeated but that does not increase runtime significantly for this data.
```{r}
idMat <- data.frame(matrix(nrow = 9, ncol = 9)) #initialize matrix
for(i in 1:9){
  for (j in 1:9){
    idMat[i,j] <- sum(pMat$rotation[,i]*eMat$vectors[,j]) #add data to matrix
  }
}
idMat #print matrix
```

We see this is essentially an identity matrix but it has the overflow errors.  Let's force those to be zero.

```{r}
idMat[abs(idMat)<0.00000001] = 0 #force really small values to be zero so you can inspect
idMat
```

We might be worried about the negative ones, but we recall that the inner product is negative one if the vectors point opposite each other but otherwise the same direction.  So we do see that these two matrix have a orthonormal column space (up to a negative direction).

1.b.i.

```{r}
heatmap(corMat, main = "Heatmap of Correlation for Glass Data", scale = "column", Rowv = NA, Colv = NA)
```

We tried for a while to get a scale for the colors on this graph and failed to find it.  We are going to use the package suggestion of `corrplot` and of course it works great and has the scale that we wanted in the defaults.  Yeah open source for the win!
```{r}
corrplot(corMat)
```

Yes, there are two solutions here but we find it important to show that sometimes the base package just doesn't have what you want.  Again with the strong correlation Ca-RI.

1.b.ii.

To look at the PCA, we'll start with the overloaded `plot` from the S3 method included with the `prcomp` function.  The scree plot follows
```{r}
plot(pMat, main = "Scree Plot for PCA of Glass", xlab = "PCA Components")
```

We don't quite see the hockey stick we expected. We also expected this to be normalized to one but here we see variances above 1 which is odd and not what we recalled in scree plots.  Lastly, we generate an excellent visualization of the PCA that includes the Type colored and the PCA directions for interpretation.


```{r}
autoplot(pMat, data = Glass, 
         color = "Type", 
         loadings = TRUE, 
         loadings.colour = 'blue',
         loadings.label = TRUE)+
  scale_color_manual(breaks = c("1","2","3","5","6","7"),
                    values =c("red","blue","green","violet","yellow","black"))
```

We like like this graphic a lot for showing off the different categories and the amount of the components in the PCA directions.

1.b.iii.

The first two components of the PCA provide the direction in the 9 dimensional space in which the most variance is explained.  We see that 27.9\% and 22.7\% of the variance are explained in these two directions.  We also see that iron (fe) and silicon (si) contribute negligibly to the first two PCAs.  This first direction of the PCA is dominated by Al, Ca, and RI. The second component is dominated by Mg and Ba.

1.b.iv.

There is some value to this PCA but only covering about 50\% of the variation has some drawbacks.  The lack of the hockeystick in the scree plot also makes me hesitant to use this PCA.  Besides all this, the dataset is relatively small dimensional at 9 dimensions.  Reducing to two or three dimensions would allow for slightly faster computations but overall, we do not think it is worth it here only getting 50\% of the variation.  No, we would not suggest using the PCA in this case.

1.c.i.
First, we will mean center the data.

```{r}
preproc.param <- Glass[2:11] %>% preProcess(method = c("center","scale"))

transformed <- preproc.param %>% predict(Glass[2:11])


```

Now we will apply the LDA transformation on the transformed data.
```{r}
ldaData <- lda(Type~., transformed)
ldaData
```

1.c.ii.  We see that the first LD1 projection picks about 81.45\% of the differences of the means.  We see that Na, Al, Si and Ca are the most important variables in our determining which class the class belongs to.
We can use the following formula to make our predictions with the LDA1.

$$
0.94*RI+1.94*Na + 1.06*Mg + 1.66*Al + 1.90*Si + 1.02*K +1.43* Ca +1.15*Ba - .05*Fe
$$

1.c.iii.

We make our predictions here
```{r}
values <- predict(ldaData, transformed)
#values$x[,1]
#ldahist(data = values$x[,1], g = Glass$Type)
```
This is the overloaded plot for the lda.  We are always curious to see these for new methods we use.
```{r}
plot(ldaData, col = as.numeric(Glass$Type))
```
A confusion table to check out the results
```{r}
table(Orignial = Glass$Type, Predicted = values$class)
```
We see in our confusion table that we did well but not perfect.  Let's see what it actually did

```{r}
mean(values$class == Glass$Type)
```

Correct about two thirds of the time, much better than guessing!

A scatterplot of the LD1 and LD2 with coloring.

```{r}
lda.data  <- cbind(transformed,predict(ldaData)$x)
ggplot(lda.data, aes(LD1,LD2)) +
  geom_point(aes(color = Type))
```

We do not see perfect separation and understand why our results are not a bit stronger.  We could not get the command `ldahist` to compile on my machine.  Instead, we went ahead and used ggplot to create something equivalent.

```{r}
ggplot(lda.data, aes(LD1)) +
  geom_histogram() +
  facet_grid(rows = "Type")
```
We see some separation in the first LDA but below in the second,  we see almost none.

```{r}
ggplot(lda.data, aes(LD2)) +
  geom_histogram() +
  facet_grid(rows = "Type")
```

2a.

Load the data and run on outlier test on the data.
```{r 2a1}
# Load the data
data("heptathlon", package = "HSAUR2")

# Apply Grubbs' test to each event
outlier_tests <- apply(heptathlon[, 1:7], 2, function(x) grubbs.test(x))

# Print out the results
outlier_tests
```
According to this test,  the competitor Launa (PNG) is identified as an outlier in multiple events: 

1. Hurdles: Highest value 16.42 is an outlier, with a p-value of 0.000436 (very strong evidence).
2. High jump: Lowest value 1.5 is an outlier, with a p-value of 0.0001698 (very strong evidence).
3. Long jump: Lowest value 4.88 is an outlier, with a p-value of 0.04594 (moderate evidence).
4. 800m run: Highest value 163.43 is an outlier, with a p-value of 0.001808 (strong evidence).

Remove Launa (PNG) from the dataset as she is consistently identified as an outlier across multiple events.

```{r 2a2}
# Remove Launa (PNG) from the dataset
heptathlon_cleaned <- heptathlon[-which(rownames(heptathlon) == "Launa (PNG)"), ]
```

2b.

```{r 2b1}
# Identify the columns for the running events
running_events <- c("hurdles", "run200m", "run800m")

# Apply the transformation: subtract each value from the maximum value in its column
heptathlon_cleaned[running_events] <- apply(
  heptathlon_cleaned[running_events],2,
  function(x) max(x) - x)
```
$$ x_{\text{transformed}} = x_{\text{max}} - x $$

By transforming all the running events by subtracting each athlete's time from the maximum recorded time in the event, we can more easily compare performance across events where higher values consistently represent better outcomes.

```{r 2b2}
# Check the transformed data for running events
head(heptathlon_cleaned[running_events])
```

2c.

```{r 2c}
# Perform PCA on the 7 event results
Hpca <- prcomp(heptathlon_cleaned[, 1:7], scale. = TRUE)

# View a summary of the PCA results
summary(Hpca)

# Check the rotation matrix 
Hpca$rotation

# Check the PCA scores 
head(Hpca$x)
```

2d.
```{r 2d}
# Visualize the first two principal components using ggbiplot
ggbiplot(Hpca, obs.scale = 1, var.scale = 1, 
         circle = TRUE) +
  ggtitle("PCA Biplot of Heptathlon Events") +
  theme_minimal() +
  theme(legend.position = "bottom")

```
PC1 (explains 61.8% of the variance): Separates competitors based on running events (800m, 200m, hurdles). Athletes with higher PC1 scores performed better in running events, while those with lower PC1 scores did better in field events (high jump, javelin).

PC2 (explains 12.8% of the variance): Mainly reflects performance in the high jump. Athletes with higher PC2 scores excelled in high jump.

Event Correlation:

1. Running events (800m, 200m, hurdles) are positively related, as their arrows point in similar directions.

2. Field events (high jump, javelin) are negatively related to running events, meaning athletes who performed well in running events tended to perform worse in field events.

Competitors: Athletes farther from the center of the plot showed more extreme performances (either very good or bad in certain events), while those closer to the center had more balanced performances across all events.

2e.
```{r 2e}
# Identify the columns for the score
heptathlon_scores <- heptathlon_cleaned$score

# Plot heptathlon score vs. PC1 projections
plot(Hpca$x[,1], heptathlon_scores, 
     xlab = "PC1 Projections", 
     ylab = "Heptathlon Score", 
     main = "Heptathlon Score vs. PC1 Projections",
     pch = 19, col = "blue")

# Add a linear regression line to see the relationship
abline(lm(heptathlon_scores ~ Hpca$x[,1]), col = "red")
```

The plot shows a negative linear relationship between the heptathlon score and PC1 projections. Since PC1 primarily reflects performance in running events, higher PC1 values indicate stronger performance in these events. The negative slope of the regression line suggests that competitors with lower PC1 projections tend to have higher overall heptathlon scores. This indicates that excelling in running events plays a significant role in achieving a higher overall score. Conversely, competitors with higher PC1 values tend to have lower total scores, meaning weaker performance in running events is associated with a lower heptathlon score.

In summary, the plot highlights that performance in running events is a key factor in determining a competitor's overall heptathlon success.









3.

Load up the data and get it going...
```{r}
housingData = read.csv("housingData-1.csv")
hd <- housingData %>%
  select_if(is.numeric) %>%
  dplyr::mutate(age = YrSold - YearBuilt,
              ageSinceRemodel = YrSold - YearRemodAdd,
              ageofGarage = ifelse(is.na(GarageYrBlt), age, YrSold - GarageYrBlt)) %>%
  dplyr::select(!c(Id,MSSubClass, LotFrontage, GarageYrBlt,
                MiscVal, YrSold , MoSold, YearBuilt,
                YearRemodAdd, MasVnrArea))
```

Next we will preform the PCA of the scaled data.
```{r}
hd.pca <- prcomp(hd,scale = TRUE)
plot(hd.pca)
```

The scree plot shows that there is a lot of information stored in that first principle component.  Let's look at the vectors and see what contributes the most to the principle components.

```{r}
autoplot(hd.pca, data = hd, 
         loadings = TRUE, 
         loadings.colour = 'blue',
         loadings.label = TRUE)
```

That is way too busy of a graph to glean much info.  We do get the percentages of the PC1 and PC2 at about 36\% which is great for such high dimensional data.  We see the age variables, garage, price and squarefootage variables in the first PCA.  In the second we see some of the bathroom variables, kitchen and garage.  This does give us some ideas about what is most important in the data.  Let's look to the correlations and see what we can get from that.

```{r}
cor(hd) %>% corrplot(method = "circle", type = "lower", tl.cex = 0.6)
```

The correlation heatmap provides an understanding of how different property characteristics are related.From the correlation heatmap we can find:

Key Drivers of Sale Price:

1. Overall Quality (OverallQual) and Above-Ground Living Area (GrLivArea) are the strongest predictors of house price, meaning better quality and larger homes tend to sell for higher prices.

2. Garage Size (GarageCars, GarageArea) also plays an important role, with larger garages linked to higher sale prices.

3. Basement Size (TotalBsmtSF) and First-Floor Area (X1stFlrSF) are highly correlated, and larger homes overall lead to higher sale prices.

Negative Influences:

House Age (age) and Time Since Remodel (ageSinceRemodel) are negatively correlated with sale price. This suggests newer homes or homes that were recently remodeled tend to sell for more.

Minimal Impact:

Features like Pool Area and Enclosed Porch don't seem to affect the sale price much.



