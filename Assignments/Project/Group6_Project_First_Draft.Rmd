---
#title: "Project:  SnOasis"
#author: "Nicholas Jacob, Yechang Qi, James Wahome, Zayne Mclaughlin"
#date: "2024-11-21"
#institute: "University of Oklahoma"
#toc: true 
output: 
  pdf_document:
    latex_engine: xelatex
header-includes:
- \usepackage{float}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(include = FALSE) #this will make none of the code appear unless we ask it to.
library(ggplot2)
library(VIM)
library(Metrics)
library(reshape2)
library(scales)
library(fastDummies)
library(glmnet)
library(tidyr)
library(forcats)
library(knitr)
library(car)
library(pls)
library(lars)
library(lubridate)
library(broom)
library(kableExtra)
library(MASS)
library(caret)
library(magrittr)
library(gridExtra)
library(dplyr)
library(stringr)
library(arules)
```
![](snoImage.png)

\vspace{2cm} 
\begin{center}
\huge \textbf{UNIVERSITY OF OKLAHOMA} \\[2ex]
\LARGE\textbf{DSA/ISE 5103 – INTELLIGENT DATA ANALYTICS}\\[4ex]

\huge \textbf{Project: SnOasis} \\[4ex]

\Large \textbf{Nicholas Jacob, Yechang Qi, James Wahome, Zayne Mclaughlin} \\[4ex]
\LARGE \textbf{Course Project Group 6} \\[10ex]

\LARGE \textbf{2024-10-21}\\[1ex]
\end{center}

\newpage

# Problem Description

Optimizing operations and identifying effective promotional strategies are key challenges for SnOasis, a chain of snow cone stands in Ada, Oklahoma. With labor costs representing the largest operational expense, understanding customer behavior and predicting sales patterns can provide critical insights to improve resource allocation and drive revenue growth.

SnOasis has collected a comprehensive dataset containing 76,219 entries representing 37,196 separate sales transactions across multiple locations. This dataset includes variables such as the time of purchase, product names (e.g., sizes like Small, Medium, Large, and flavors like Lime, Cream, Kitkat), quantities sold, prices, and subtotals. Despite the richness of this information, it remains underutilized in uncovering meaningful patterns or informing strategic decision-making. Since we don’t have direct labor data, we would like to estimate labor needs using sales patterns by analyzing transaction volumes and sales trends to infer peak demand periods and staffing requirements.

One area of focus is examining the associations between purchased items to better understand customer purchasing behavior. Identifying patterns in how products are commonly bought together can provide insights into customer preferences and spending habits. This analysis can help SnOasis develop targeted promotional strategies that not only enhance the customer experience but also drive sales and increase profitability. By leveraging these associations, SnOasis can make data-driven decisions to optimize product offerings and marketing campaigns.

Another important area is the prediction of sales based on time and day. Analyzing how sales vary across different times of the day and week allows us to create a demand schedule, helping SnOasis optimize staffing levels. By aligning labor with peak demand periods and reducing overstaffing during slower times, SnOasis can minimize operational costs while maintaining excellent service.

By leveraging this dataset, along with association rules and predictive modeling, this project aims to provide actionable insights that will help SnOasis streamline operations, reduce costs, and better serve its customers.

# Exploratory Data Analysis

Our dataset needs some cleaning before analysis. Thanks to SnOasis's real-time recording system, there are no missing values, but several issues still require attention. First, we address outliers, such as negative values in Quantity or Final Price that indicate returns or corrections. These transactions, along with their corresponding original entries, are identified and removed. Unusually high prices or quantities are also reviewed and capped if necessary. Category data, including Staff, Location, and Product Names, is formatted for analysis, with similar products grouped together to simplify the dataset. Additionally, date and time information is cleaned by fixing any irregular symbols and extracting useful details, such as the day of the week and hour of the day, to enhance the analysis.

```{r dataload}
df = read.csv("https://github.com/nurfnick/SnOasis/raw/refs/heads/main/snoasisData.csv")
tail(df)
```


```{r}
#Time needs to be cleaned up.  Made it into datetime.
df$dt <- make_datetime(year(mdy(df$Date)), 
              month(mdy(df$Date)),
              day(mdy(df$Date)),
              hour(hms(df$Time)),
              minute(hms(df$Time)))
# Remove '?' from Time column
df$Time <- gsub("\\?", "", df$Time)
```

```{r}
dfNumeric <- df %>% 
  dplyr::select(where(is.numeric))
glimpse(dfNumeric)
dfNumeric <- dfNumeric %>%
  rename(
    Receipt_number = `Receipt.number`,
    Quantity = Quantity,  # this one stays the same
    Price = `Price..USD.`,
    Discount = `Discount..USD.`,
    Subtotal = `Subtotal..USD.`,
    Total_tax = `Total.tax.collected..USD.`,
    Final_price = `Final.price..USD.`,
    Cost_price = `Cost.price`
  )
dfNumeric <- dfNumeric %>% 
  mutate(across(everything(), ~ ifelse(. < 0, 0, .)))
```
```{r}
dfFactor <- df %>% 
  dplyr::select(where(is.character))%>%
  mutate_all(factor)
glimpse(dfFactor)
dfFactor <- dfFactor %>%
  rename(
    Tax_info = Tax.Info.Available,
    Tax_exempt = Is.Tax.Exempt
  )

```

```{r}
Q1<-function(x,na.rm=TRUE) {
quantile(x,na.rm=na.rm)[2]
}
Q3<-function(x,na.rm=TRUE) {
quantile(x,na.rm=na.rm)[4]
}
```

```{r}
myNumericSummary <- function(x){
  c(length(x), n_distinct(x), sum(is.na(x)), mean(x, na.rm=TRUE),
  min(x,na.rm=TRUE), Q1(x,na.rm=TRUE), median(x,na.rm=TRUE), Q3(x,na.rm=TRUE),
  max(x,na.rm=TRUE), sd(x,na.rm=TRUE))
}
```

```{r}
# Summary function
numericSummary <- dfNumeric %>%
  summarise_all(myNumericSummary) %>%
  # Adding descriptive statistics to the numeric summary table
  cbind(stat = c("n", "unique", "missing", "mean", "min", "Q1", "median", "Q3", "max",
                 "sd")) %>%

  tidyr::pivot_longer(cols = "Receipt_number":"Cost_price", names_to = "variable", 
                      values_to = "value") %>%
  tidyr::pivot_wider(names_from = stat, values_from = value) %>%
  dplyr::mutate(
    missing_pct = 100 * missing / n,
    unique_pct = 100 * unique / n
  ) %>%
  # Selecting and ordering columns
  dplyr::select(variable, n, missing, missing_pct, unique, unique_pct, everything())

# Limiting the number of digits in the table and using scientific notation
options(digits = 2, scipen = 0)
```

```{r, include = TRUE, fig.pos="H"}
# Display the Descriptive Summary of Numeric Variables
numericSummary %>%  
  kable(digits = 2, format = "latex", booktabs = TRUE, 
        caption = "Descriptive Summary of Numeric Variables") %>%
  kable_styling(font_size = 12,latex_options = c("H", "scale_down")) %>%
  row_spec(0, bold = TRUE) %>% 
  row_spec(1:nrow(numericSummary), extra_latex_after = "\\addlinespace[0.5em]")
```

```{r}
# Function to get the mode
getmode <- function(v) {
  tbl <- table(v)
  return(names(which.max(tbl)))  # 1st mode
}

# Function to get the mode frequency
getmodeCnt <- function(v) {
  tbl <- table(v)
  return(max(tbl))  # 1st mode frequency
}
```

```{r}
# Define categorical summary function
myCategoricalSummary <- function(x) {
  c(
    length(x),
    sum(is.na(x)),
    n_distinct(x),
    getmode(x), getmodeCnt(x)
  )
}

# Apply the categorical summary function to all factors in trainFactor
factorSummary <- dfFactor %>%
  summarise_all(myCategoricalSummary)

# Add column titles to the factorSummary table
factorSummary <- cbind(
  stat = c("n", "missing", "unique", "mode", "mode_freq"),
  factorSummary
)

# Reshape the data and calculate percentages for missing and unique values
factorSummaryFinal <- factorSummary %>%
  tidyr::pivot_longer(cols = "Date":"Tax_exempt", 
                      names_to = "variable", values_to = "value") %>%
  tidyr::pivot_wider(names_from = stat, values_from = value) %>%
  dplyr::mutate(
    missing_pct = 100 * as.numeric(missing) / as.numeric(n),  # Calculate missing percentage
    unique_pct = 100 * as.numeric(unique) / as.numeric(n),  
  ) %>%
  dplyr::select(variable, n, missing, missing_pct, unique, unique_pct, everything())
# Set display options for precision and formatting
options(digits = 3, scipen = 99)
```

```{r ,include = TRUE, fig.pos = "H"}
# Display the final summary table with kable
factorSummaryFinal %>% 
  kable(digits = 2, format = "latex", booktabs = TRUE, 
        caption = "Descriptive Summary of Categorical Variables") %>%
  kable_styling(font_size = 7,latex_options = c("H")) %>%
  row_spec(0, bold = TRUE) %>%  # Make header bold
  row_spec(1:nrow(factorSummaryFinal), extra_latex_after = "\\addlinespace[0.25em]")
```

The numeric variables in the Table 1 provide a clear understanding of transaction details, including the quantity of items sold, their prices, and the financial aspects of each sale. Notably, the Receipt_number variable highlights 76,219 entries across 37,196 unique transactions, indicating multi-line transactions within single receipts. The Quantity variable shows that most transactions involve one or two items, as reflected by a median value of 1 and a maximum of 40. This suggests that bulk purchases are uncommon, likely due to the nature of the business, which caters to individual customers or small groups. The Price variable ranges from 0 to 104.5, with most items priced under \$3.50 (Q3 = 3.5). This aligns with expectations for snow cone sales, where individual items are typically low-cost. Discounts are represented by a binary Discount variable, where the vast majority of transactions do not include discounts (mode = 0). Meanwhile, the Subtotal, Total_tax, and Final_price variables confirm that most transactions are small in scale, with medians of \$3.75, \$0.09, and \$2.20, respectively. The high variability in prices and subtotals may be influenced by factors like product size, add-ons, or customer preferences.

Table 2 also provide important insights into transaction details. The Date variable captures sales across 236 unique days, with the highest number of transactions recorded on May 6, 2023 (671 sales). This could indicate a particularly busy day for the business, possibly tied to a seasonal or promotional event. The Time variable contains over 21,000 unique timestamps, with a most frequent transaction time of 2:48:12 PM, though this frequency (37 transactions) suggests that transactions are fairly evenly distributed throughout the day. The Staff variable reveals that "SnOasis Main" handles the bulk of the transactions, accounting for 38,804 sales. This indicates that this location likely serves as the primary or busiest stand in the chain. The Name variable highlights the popularity of specific products, with "Medium" being the most frequently sold product, appearing 16,391 times. This provides insights into customer preferences, which can inform inventory and promotion decisions.

While this summary provides an initial understanding, visualizing the data offers deeper insights into customer behavior. To further explore patterns, we investigate sales trends across different timeframes, including by hour, weekday, and month.

```{r,include = TRUE, fig.height=4}
# First plot: Sales by Hour
p1 <- df %>%
  mutate(hour = factor(hour(dt), levels = c(9,10,11,12,1,2,3,4,5,6,7,8))) %>%
  ggplot(aes(x = hour)) +
  geom_bar() +
  labs(title = "Sales by Hour") +
  theme_minimal() +
  theme(
    text = element_text(size = 10),          # Set all text elements to size 10
    plot.title = element_text(size = 10),   # Specific adjustment for plot title
    axis.text = element_text(size = 10),    # Axis labels
    axis.title = element_text(size = 10)    # Axis titles
  )

# Second plot: Sales by Weekday
df$weekdays <- factor(weekdays(df$dt), 
                      levels = c("Sunday", "Monday", "Tuesday", "Wednesday", 
                                 "Thursday", "Friday", "Saturday"))

p2 <- df %>%
  ggplot(aes(x = weekdays)) +
  geom_bar() +
  labs(title = "Sales by Weekday") +
  theme_minimal() +
  theme(
    text = element_text(size = 10),
    plot.title = element_text(size = 10),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 10)
  )

# Third plot: Sales by Month
p3 <- df %>% 
  mutate(month = factor(month(dt, label = TRUE, abbr = TRUE),
                        levels = month.abb)) %>%
  ggplot(aes(x = month)) +
  geom_bar() +
  labs(title = "Sales by Month") +
  theme_minimal() +
  theme(
    text = element_text(size = 10),
    plot.title = element_text(size = 10),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 10)
  )

# Combine the three plots into one figure with a title
grid.arrange(
  p1, p2, p3, 
  ncol = 1, 
  top = textGrob(
    "Figure 1: Summary of Sales by Hour, Weekday, and Month", 
    gp = gpar(fontsize = 10)
  )
)
```

The Sales by Hour plot shows a peak in sales between 2:00 AM and 4:00 AM, indicating that most transactions occur during these early morning hours. The Sales by Weekday plot reveals consistent sales across the week, with only a slight dip on Sundays, suggesting steady demand without a strong weekday or weekend effect. The Sales by Month plot highlights higher sales from April to July, followed by a decline from August to October, hinting at seasonal trends with peak activity in spring and early summer and a slower period in late summer.
 
```{r include = TRUE, fig.height=4}
df %>%
  filter(Name != "" & Name != "5.31") %>%  # Exclude blank and "5.31" product names
  ggplot(aes(x = Name, y = Price..USD.)) +
  geom_boxplot(
    fill = "skyblue", 
    color = "darkblue", 
    outlier.color = "red", 
    outlier.size = 1
  ) +  # Set the size of the outliers (dots)
  labs(
    title = "Figure 2: Distribution of Sales by Product", 
    x = "Product Name", 
    y = "Price (USD)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(
      size = 10,           # Set the font size to 10
      hjust = 0.5          # Center the title
    ),
    axis.text.x = element_text(angle = 90, hjust = 1)  # Rotate x-axis labels
  )
```

This plot, showing the Distribution of Sales by Product, illustrates the variation in sales prices across different products. Most products have a narrow price range clustered near the bottom of the plot, indicating relatively low and consistent prices. However, there are a few products with a wider range and several high-price outliers (indicated by red dots) — for example, products like "Gift Card," "Large," "Kiddie," and "Skittles" have higher price variability and occasional outliers reaching above $25. This suggests that while most items are low-cost, a few products are occasionally sold at higher prices, possibly due to different sizes, premium options, or special product variations.
```{r}
cap_outliers <- function(x) {
  q1 <- quantile(x, 0.25, na.rm = TRUE)
  q3 <- quantile(x, 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  x <- ifelse(x < lower_bound, lower_bound, x)
  x <- ifelse(x > upper_bound, upper_bound, x)
  return(x)
}

# Apply to numeric columns
dfNumeric <- dfNumeric %>%
  mutate(across(everything(), cap_outliers))
```

# Description of modeling approach & Initial results

## Association on Sales

We aimed to explore whether there are any associations between purchases made during different parts of the day. Specifically, we wanted to investigate if certain buying patterns exist, such as whether a customer who purchases a large item is likely to also buy a medium and candy. To uncover these potential relationships, we utilized the arules package, which is designed to identify patterns and associations within transactional data. This analysis provides insights into customer behavior that can inform targeted promotions and bundling strategies.


```{r}
df <- df %>%
  mutate(
    # Convert weekdays to an ordered factor to maintain consistent weekday order on the x-axis
    weekdays = factor(weekdays(dt), 
                      levels = c("Sunday", "Monday", "Tuesday", "Wednesday", 
                                 "Thursday", "Friday", "Saturday")),
    month = factor(month(dt))
  )
dfByTrans <- df %>%
  group_by(Receipt.number) %>%
  summarize(names = list(unique(str_trim(Name)))) %>%
  ungroup()

```
With the data compiled at the transaction level, we examine the top 10 most common purchases, surprisingly large is below adding lime.

```{r, include = TRUE}
trans <- as(dfByTrans$names, "transactions")

itemFrequencyPlot(
  trans, 
  topN = 10,                            
  type = "absolute",                     # Use absolute frequencies
  main = "Figure 3: Item Frequency Plot",          # Title              
  cex.names = 0.7,                       # Set axis text size (relative to default)
  cex.main = 0.8 ,
  font.main = 1 
)

```

```{r}
rules <- apriori(trans, 
                 parameter = list(supp=0.05, conf=0.25, 
                                  maxlen=10, 
                                  minlen = 2,
                                  target= "rules"))
```
```{r, include = TRUE}
# Convert the rules to a data frame
rules_df <- as(rules, "data.frame")

# Create and style the table
rules_df %>%
  kbl(
    digits = 3,  # Round numeric columns to 3 decimal places
    caption = "Association Rules Summary"
  ) %>%
  kable_styling(
    full_width = FALSE, 
    position = "center", 
    bootstrap_options = c("striped", "hover", "condensed")
  ) %>%
  row_spec(0, bold = TRUE)  # Bold the header row

```
Next, we use this analysis to predict consumer behavior and identify potential opportunities for targeted promotions. The association rules generated from the Apriori Algorithm allow us to uncover patterns in how customers combine items during their purchases. By analyzing these rules, we can gain insights into customer preferences and shopping habits, which can guide both operational decisions and marketing strategies.

From Table 3, we observe that items like Lime, Cream, Kiddie, Small, and Medium are interconnected, creating 11 association rules. However, these rules generally have low support, meaning that the combinations occur in a smaller percentage of transactions. Despite this, these rules still provide valuable insights into consumer behavior and preferences. For example, a rule like {Large} ⇒ {Lime}, with a confidence of 34%, suggests that customers who purchase a "Large" are relatively likely to add "Lime" to their order. This highlights an opportunity to promote "Lime" as an add-on for "Large" orders, which could drive additional revenue or encourage upsizing.

Similarly, the connection between "Kiddie" and other sizes like "Medium" suggests that customers may respond well to promotions offering upgrades or discounts on additional purchases. These insights allow SnOasis to think strategically about bundling and promotions. For instance, running a limited-time offer, such as a free "Lime" with every "Large" purchase, could capitalize on the existing association between these items. Additionally, offering a second "Medium" size for the price of a "Small" could encourage customers to purchase more and potentially shift their buying preferences toward larger sizes in the long term.

While the low support of these rules limits their overall impact, they still provide a basis for targeted promotions and marketing efforts. By leveraging these insights, SnOasis can experiment with data-driven strategies to influence customer behavior and maximize revenue.

## Methodology for Apriori Algorithm

In order to understand the transactions presented in the data, we applied the Apriori algorithm.  This allowed us to examine the most frequent items that were purchased together.  The algorithm uses several parameters that required tuning through the use of the guess and check.  We desired to create some rules but to not overwhelm the reader with too many.  A support of 5\% and a confidence of 25\% achieved this goal.

## Regression for Time of Day

Our business partner would love to have a demand schedule based on the time of day and week.  We will build a model for that.


The regression model used to predict total sales can be expressed mathematically as:

\[
\text{Total Sales} = \beta_0 + \beta_1 \cdot \text{Month} + \beta_2 \cdot \text{Weekday} + \beta_3 \cdot \text{Staff} + \beta_4 \cdot \text{Hour} + \beta_5 \cdot \text{Minute} + \beta_6 \cdot (\text{Hour} \cdot \text{Minute}) + \epsilon
\]

Where:

- \(\beta_0\): Intercept

- \(\beta_1\): Coefficients for the **Month** variable, capturing seasonal effects.

- \(\beta_2\): Coefficients for the **Weekday** variable, capturing variations across weekdays.

- \(\beta_3\): Coefficients for the **Staff** variable, accounting for the effect of different locations/staff.

- \(\beta_4\): Coefficients for the **Hour** variable, capturing time-based variations in sales.

- \(\beta_5\): Coefficients for the **Minute** variable, representing changes within each hour.

- \(\beta_6\): Coefficients for the **interaction term** (\( \text{Hour} \cdot \text{Minute} \)), capturing the combined effect of specific hours and minutes.

- \(\epsilon\): Error term, accounting for unobserved variability.
```{r}
library(dplyr)
library(lubridate)

# Group data by 15-minute intervals and summarize
#I want to group the data by 15 minute chunks so that I can accurately predict how to allocate workers. 
dfBy15min <-df %>%
  group_by(group15min = cut(dt, "15 min"),Staff) %>% #Include the staff variable to analyze the impact of staff on sales ]
  summarize(names = list(unique(str_trim(Name))),totalSales = sum(Final.price..USD.), month, weekdays)%>% #Collect unique item names sold during each interval
  mutate(hour = factor(hour(group15min)), #Extract the hour of each 15-minute interval as a factor
    min = minute(group15min) #Extract the minute of each 15-minute interval as a numeric value
  )%>%
  ungroup()

#Fit a linear regression model to predict total sales
  #summarize(totalSales = sum(Final.price..USD.)) %>%
linear_sales_model <- lm(
  totalSales ~ month + weekdays + Staff + hour + min + hour:min, #Use these predictors:
  #month: Seasonal effects (e.g., higher sales in December).
  #weekdays: Captures variations between weekdays and weekends.
  #Staff: Helps understand if certain staff members influence sales.
  #hour and min: Time-based features to capture daily demand fluctuations.
  #hour:min: Interaction term to detect how specific time combinations (e.g., 10:15 AM) affect sales.
  data = dfBy15min) #Use the grouped and aggregated dataset for modeling

# Tidy the model output and add significance stars
significant_model_table <- linear_sales_model %>%
  tidy() %>%
  mutate(
    significance = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      p.value < 0.1 ~ ".",
      TRUE ~ ""
    )
  ) %>%
  filter(significance != "") # Keep only rows with significance stars

# Print the filtered table
print(significant_model_table)
```

```{r, include = TRUE}
significant_model_table %>%
  select(term, estimate, std.error, statistic, p.value, significance) %>%
  kbl(
    digits = 3, 
    format = "latex",          # LaTeX format for PDF
    booktabs = TRUE,           # Use professional-looking table style
    caption = "Linear Regression Results with Significance Stars"
  ) %>%
  kable_styling(
    latex_options = c("hold_position", "scale_down"), # Keep the table near and scalable
    font_size = 10                                   # Adjust font size
  ) %>%
  row_spec(0, bold = TRUE) %>%                       # Bold the header row
  column_spec(1, bold = TRUE, width = "8em")         # Bold the first column for emphasis

```

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
train(totalSales ~ month + weekdays + Staff + hour + min + hour:min,
      data = dfBy15min,
      method = "lm",
      trControl = fitControl)
```
The linear regression model provides a detailed understanding of the factors influencing sales at SnOasis. Table 4 presented includes only statistically significant results (p-values less than 0.1), ensuring the focus is on meaningful predictors. These insights reveal patterns related to time, staffing, and seasonal trends, all of which are critical for optimizing operations.

The analysis of monthly effects shows that sales decline significantly in November, with a strong significance level (p = 0.001), indicating a clear seasonal drop in demand. October also shows a decrease in sales, but the effect is only marginally significant (p = 0.085). These findings highlight the seasonal nature of sales, particularly in November, and suggest the need for strategies to counteract lower demand during these months.

Variations in sales across weekdays are also evident. Mondays and Tuesdays show significantly lower sales, with very strong significance levels (p < 0.001). Wednesday also experiences a smaller but still notable decline in sales (p < 0.001). On the other hand, Saturdays show a modest but meaningful increase in sales (p = 0.018), consistent with higher weekend traffic. These patterns underscore the importance of tailoring staffing levels and promotional efforts to meet day-specific demand.

Staffing has a significant impact on sales, with SnOasis Main and SnOasis East locations driving much higher sales compared to the baseline location, both showing very strong significance levels (p < 0.001). This indicates that these locations are major contributors to overall sales, likely due to higher foot traffic or operational efficiency. These insights can guide resource allocation, helping SnOasis focus on high-performing locations while identifying improvement areas for others.

Hourly effects reveal substantial fluctuations in sales throughout the day. Early morning hours, such as Hour 2 and Hour 3, see significant increases in sales, both with very strong significance levels (p < 0.001). Conversely, late morning and midday hours, such as Hour 11, experience sharp declines in sales, also with very strong significance (p < 0.001). These insights are critical for optimizing staffing schedules, ensuring sufficient coverage during peak hours and avoiding overstaffing during slower periods.

The minute variable shows a slight decrease in sales as the minute progresses within each hour, with moderate significance (p = 0.012). While the effect size is small, it may reflect the natural pacing of transactions or customer arrivals.

Interaction terms between hours and minutes provide further insights into sales variations within 15-minute intervals. For example, during Hour 2, specific minutes show a small positive effect on sales (p = 0.035), while Hours 3 and 5 show slight decreases during certain intervals, with moderate significance (p = 0.026 and p = 0.041, respectively). These nuanced interactions highlight subtle time-based variations that can inform more precise adjustments in operations.

In conclusion, this table provides a focused view of the significant predictors of sales, with p-values indicating their reliability. These results allow SnOasis to strategically plan staffing, promotions, and operational improvements, ensuring resources are aligned with customer demand to maximize efficiency and profitability.

\newpage

# Team Allocation:

- **Nicholas Jacob**: Led the association rule mining using the Apriori algorithm and conducted code testing. Interpreted the association rule results.

- **Yechang Qi**: Wrote the problem description and performed exploratory data analysis. Interpreted the linear model results.

- **Zayne Mclaughlin**: Led the linear modeling and generated the results.



\newpage
# Appendix: Data quality report
```{r,include = TRUE}
glimpse(dfNumeric)
glimpse(dfFactor)
```

# Variable explanation for “SnOasis” file：


- **Date**: The date of the transaction, formatted as MM/DD/YYYY.

- **Time**: The time of the transaction, indicating when the sale was processed (e.g., 7:50:56 PM).

- **Staff**: Identifier for the staff member or location (e.g., "SnOasis Main" or "SnOasis East") that processed the transaction.

- **Receipt number**: Unique identifier for each transaction, acting as a receipt or transaction ID.

- **Name**: Name of the item sold (e.g., "Gift card," "Candy Bar").

- **Variant**: Any specific variation of the item (this field appears mostly blank).

- **Unit**: Likely denotes unit type or measurement, though it's mostly empty here.

- **Quantity**: The number of units sold in the transaction.

- **Price (USD)**: Price per unit in USD before any discounts.

- **Discount (USD)**: Discount applied to the item in USD.

- **Subtotal (USD)**: Total amount before tax, accounting for any discounts.

- **Tax Info Available**: Indicates if tax information is available (e.g., "Yes" or "No").

- **Is Tax Exempt**: Whether the transaction is exempt from taxes (e.g., "Yes" or "No").

- **Total tax collected (USD)**: Amount of tax collected in USD for the transaction.

- **Final price (USD)**: Total amount paid after taxes and discounts.

- **SKU**: Stock-keeping unit identifier for the item, a unique code for tracking inventory.

- **Barcode**: Barcode of the item, for scanning purposes (appears mostly empty).

- **Cost price**: Cost price for the item, representing the cost to the business (appears mostly zero here).

- **Comment**: Field for any additional notes or comments about the transaction.

```{r, eval = FALSE}
df_sale = read.csv("https://github.com/nurfnick/SnOasis/raw/refs/heads/main/snoasisData.csv")
df_sale <- df_sale %>%
  rename(
    Time_of_Sale = `Time.of.Sale`,
    Sale_Description = `Sale.Description`,
    Location_of_Sale = `Location.of.Sale`
  )
glimpse(df_sale)
```
# Variable explanation for “SnOasisSale” file：

- **Day**: The specific date of the sale event, formatted as MM/DD/YYYY.

- **Sale Description**: A detailed description of the sale event, including any promotional offers or special deals (e.g., "Buy 1 get 1 free" or "Free Toppings").

- **Time of Sale**: The timeframe during which the sale event is active (e.g., "11 AM - 1 PM" or "All day").

- **Location of Sale**: Specifies the location of the sale event, such as "Mobile Trailer," "East," or "Main."


#Modeling Summary
 -**1. Data Preparation**:
    Objective: Ensure the dataset is clean and well-structured to support analysis and development.
    We removed outliers (e.g., negative prices/quantities).
    Then we standardized variables and enriched date-time with features (e.g., hour, weekday) and grouped data into 15-minute intervals for regression analysis.

 -**2. Exploratory Data Analysis**:
    Though analysis we found peak sales between 2:00–4:00 AM, with Fridays being busiest as well as determine popular products: "Medium," "Large," and add-ons like "Lime."
    Then explore the seasonal trends such as High sales in spring/summer, and declining late summer.

 -**3. Association Rule Mining**:
    We uncovered relationships between purcahsed items to help with selling strategies. 
    We found that customers frequently pair sizes and add ons (e.g., "Lime" with "Medium").

 -**4. Regression Modeling**:
    We delevolped a predictive model based on time, day, and staff levels for the increase in sales. 
    We built a linear regression using time, day, and staffing levels.
    We found that interaction efffects revealed great predictive oportunities for predicting sales.

 -**5. Insights**:
    We found large impacts from staffing and need to focus resources on early morning peaks, especially weekends and we found that promotions can leverage popular add-ons and seasonal campaigns.

 -**Future Directions*:
    Next we want to test advanced models (e.g., time-series analysis) to enchance sales forecasts and explore clustering models for inventory management.






