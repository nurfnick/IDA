---
title: "Assignment 8 Clustering"
author: "Nicholas Jacob"
date: "2024-11-15"
output: pdf_document
---



```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_chunk$set(include = FALSE) #this will make none of the code appear unless we ask it to.
library(ggplot2)
library(Metrics)
library(reshape2)
library(scales)
library(fastDummies)
library(kableExtra)
library(MASS)
library(caret)
library(magrittr)
library(dplyr)
library(glmnet)
#library(outliers)
#library(ggbiplot)
#library(GGally)
#library(gridExtra)
library(tidyr)
library(forcats)
library(knitr)
library(car)
library(pls)
library(lars)
library(lubridate)
library(cluster)    #provides more cluster algorithms than base R (e.g. PAM)
library(useful)     #provides a plot function for clusters and "FitKMeans" and "PlotHartigan" functions
library(NbClust)  
library(dbscan)
```
# Wine Quality Reds

```{r }
df = read.csv("winequality-red.csv", sep = ";")
head(df)
```
The wine quality dataset came from the UCI Machine Learning Repository linked directly [here](https://archive.ics.uci.edu/dataset/186/wine+quality).  We see there are `r length(df)` variables and `r nrow(df)` entries.  All of the entries are numerical except that the quality of the wine is listed as a factor ranging from 0 to 10.  We will try to see if we can cluster and find the quality.

```{r}

dfScaled<-scale(df[,!(names(df) %in% c('quality'))])


dfKM <- kmeans(dfScaled,3, nstart=20)  
```

I removed the quality from the data before running it through the cluster analysis.  I ran `kmeans` with several starts but it always went to `r dfKM$iter`.  While I was expecting 10 because of the quality.  

This graph here uses the two qualities of wine I am most familiar with, sulfates and alcohol.

```{r, include = TRUE}
ggplot(data = df, aes(x = sulphates, y = alcohol, colour = factor(dfKM$cluster), shape = factor(quality)))+
  geom_point() +
  labs(title = "KNN Means Cluster",colour = "Cluster", shape = "Quality")

```
```{r, include = TRUE}
wssplot <- function(data, nc=15){                    

  par(mfrow=c(1,2))
  
  wss <- NULL  
  pctExp <-NULL
  
  for (k in 1:nc)
  {
     kclus <- kmeans(data, centers=k)
     wss[k] <- kclus$tot.withinss      #store the total within SSE for given k
     pctExp[k] <- 1-wss[k]/kclus$totss
  }
  
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")

  plot(1:nc, pctExp, type="b", xlab="Number of Clusters",
       ylab="Pct Explained")
  
  par(mfrow=c(1,1))
}


wssplot(dfScaled,nc=30)
```

Utilizing the graphic from the lecture notes, we do not note a sharp elbow in either of the graphics looking at the knn-means.  We look at a few more approaches. 

```{r, include = TRUE}
clusFit<-FitKMeans(dfScaled,max.clusters=30,nstart=20)   #evaluates k using the "Hartigan" rule

PlotHartigan(clusFit)

```

We see that by following Hartigan's Rule, we end up with some overfitted model.  I am not surprised having had a friend study for his sommelier.  It was a great summer to be around his house, always excellent bottles on hand.  I could never tell the difference between good and excellent but was a happy drinking partner.

Lastly, I examine the plot overloaded function available in `useful` library.  With PCA we do see how the clusters were chosen if not what we had hoped for.

```{r, include = TRUE}
plot(dfKM,data=dfScaled) 
```
I'll fit with 6 clusters also just to see what that looks like just to satisfy my curiosity on what we expect the fit to be.
```{r, include = TRUE}
dfKM6 = kmeans(dfScaled,6)
plot(dfKM6,data=dfScaled) 
table(dfKM6$cluster, df$quality)
```
Both the visualization and the table show me little to expect that this arbitrary fixing at 6 levels is appropriate.  Therefore I have left it at 3.



Next, I look at hierarchical clustering.  We see the tree assignments.  Cutting at 6 for the differing levels of quality of wine, I can not interpret if the clustering method has returned any indication of quality.

```{r, include = TRUE}
di <- dist(dfScaled, method="euclidean")   # with hiearchical clustering, only need distance matrix

hc <- hclust(di, method="ward.D")
#plot(hc, labels=FALSE)

df$hcluster <- as.factor(cutree(hc, k=6))   

#and a quick check to see how our 'natural clusters' align with the species data
table( df$quality, df$hcluster)
```

```{r, include = TRUE}
ggplot(data = df, aes(x = sulphates, y = alcohol, colour = factor(hcluster), shape = factor(quality)))+
  geom_point() +
  labs(title = "Hierarchical Cluster",colour = "Cluster", shape = "Quality")
```

Perhaps the cutting at 6 was arbitrary?  We repeat with more, 19 seemed to work best.
  
```{r, include = TRUE}
df$hcluster <- as.factor(cutree(hc, k=19))  
table( df$quality, df$hcluster)
```




Lastly, I attempt `dbscan`.  I tweaked the parameters of `eps` and `minPts` a bit but was never truly satisfied that most would find clusters.  I included the output from this method because I found it condescending as I was tuning.

```{r, include = TRUE}
dbScanOut <- dbscan(dfScaled, eps =  1, minPts = 10)
dbScanOut
```

Next, I recreate the graph I started with with the new dbscan clustering.

```{r, include = TRUE}
ggplot(data = df, aes(x = sulphates, y = alcohol, colour = factor(dbScanOut$cluster), shape = factor(quality)))+
  geom_point() +
  labs(title = "DBScan Cluster",colour = "Cluster", shape = "Quality")
```

I see no obvious clusters here.  



Lastly, I will revisit kmeans and attempt to interpret the outputs.  We see a bit here how the principle components work to get the clusters but no real obvious patterns, hence all the trouble finding clusters in this data.  This is the best we can hope for!



```{r, include = TRUE}
clusInfo<-data.frame(dfKM$centers,dfKM$size)
clusInfo

```

We see the first group with high acidity and sulfates but low ph.  The second group has a high ph.  The last cluster has low alcohol and high sugars (in brewing this is a sign of not enough time fermenting).
